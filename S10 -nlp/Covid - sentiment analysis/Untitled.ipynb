{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb268cb",
   "metadata": {},
   "source": [
    "# <center><font color='blue'>SENTIMENT ANALYSIS: COVID</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ef0e5",
   "metadata": {},
   "source": [
    "## Tabla de contenido\n",
    "- [1 - Objetivos](#1)\n",
    "- [2 - Librerías necesarias](#2)\n",
    "- [3 - Carga y visualización de datos](#3)\n",
    "- [4 - Pre-procesamiento de datos](#4)\n",
    "    - [4.1. - Datos faltantes](#4.1)\n",
    "    - [4.2. - Data Categóricos](#4.2)\n",
    "    - [4.2. - Balanceo de clases](#4.3)\n",
    "    - [4.4. - Pre-Procesamiento especial para NLP](#4.4)\n",
    "- [5 - Modelos](#5)\n",
    "- [6 - Ajuste de hiperparámetros](#6)\n",
    "- [7 - Conclusiones](#7)\n",
    "- [8 - Referencias](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd6143",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1. Objetivos\n",
    "\n",
    "Practicar con un problema de procesamiento del lenguaje natural.\n",
    "<br>\n",
    "Aquí, dado un conjunto de tweets, analizar si el sentimiento es positivo o negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ea4fd",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2. Librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b25607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# que no se impriman info y warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6215602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers,callbacks,models,Sequential,losses\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import os,random\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2a8f2",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3. Carga y visualización de datos\n",
    "\n",
    "Tenemos 2 datasets, uno para entrenamiento y otro para test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d5087c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pandas = pd.read_csv('data/Corona_NLP_train.csv',encoding='latin-1')\n",
    "test_data_pandas = pd.read_csv('data/Corona_NLP_test.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9cd1da54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc18ed5",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4. Pre-procesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f705d5",
   "metadata": {},
   "source": [
    "<a name=\"4.1.\"></a>\n",
    "### 4.1. Datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e27a2e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos faltantes train:\n",
      " UserName            0\n",
      "ScreenName          0\n",
      "Location         8590\n",
      "TweetAt             0\n",
      "OriginalTweet       0\n",
      "Sentiment           0\n",
      "dtype: int64 \n",
      "\n",
      "Datos faltantes test:\n",
      " UserName           0\n",
      "ScreenName         0\n",
      "Location         834\n",
      "TweetAt            0\n",
      "OriginalTweet      0\n",
      "Sentiment          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Datos faltantes train:\\n {train_data_pandas.isnull().sum()} \\n')\n",
    "print(f'Datos faltantes test:\\n {test_data_pandas.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab1e77",
   "metadata": {},
   "source": [
    "Vemos que no hay datos faltantes en las columnas que nos interesan (OriginalTweet y Sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf849bb",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### 4.2. Datos categóricos \n",
    "\n",
    "Nos interesaremos en las columnas OriginalTweet y Sentiment; a su vez veremos las distintas opciones de esta última columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a07e94b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',\n",
       "       'Extremely Positive'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pandas['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e5299",
   "metadata": {},
   "source": [
    "Convertiremos esta columna a valores numéricos; a su vez no nos interesa ser tan específicos respecto a si un sentimiento es postivo o extremadamente positivo, más bien distinguiremos entre positivo y negativo. Los neutrales los consideraremos positivos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5bc1e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':1,'Extremely Positive':1}\n",
    "train_data_pandas['Sentiment'] = train_data_pandas['Sentiment'].map(label_map)\n",
    "test_data_pandas['Sentiment'] = test_data_pandas['Sentiment'].map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58eb476",
   "metadata": {},
   "source": [
    "Chequeamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d725ef6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pandas['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab485e",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### 4.3.  Balanceo de clases\n",
    "\n",
    "Veamos si las clases están balanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db59aa35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25759\n",
       "0    15398\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pandas['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c58118f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625871662171684"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25759/(25759+15398)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3636a",
   "metadata": {},
   "source": [
    "Tenemos un desbalance moderado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0baa4bd",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"> </a>\n",
    "### 4.4. Pre-procesamiento especial para NLP\n",
    "\n",
    "Vamos a pe-procesar el texto de OriginalTweet, para ello:\n",
    "\n",
    "\n",
    "- Quitaremos las stop-words\n",
    "- Quitaremos algunos caracteres especiales, como \"@\"\n",
    "- Aplicaremos Lemmatization\n",
    "\n",
    "\n",
    "<b>Nota:</b> Habría que quitar también las puntuaciones, llevar todo a minúscula y tokenizar, pero eso lo haremos luego con TextVectorization.\n",
    "\n",
    "Descargaremos e imprimiremos para ver las stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "063ff6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# View stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb2995",
   "metadata": {},
   "source": [
    "También necesitaremos \"punkt\" y \"wordnet\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb42d306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8742b9e5",
   "metadata": {},
   "source": [
    "Quitamos las stop words y aplicaremos la lematización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "07e4e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Función para quitar palabras de parada y lematizar un texto\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Aplicar la función a la columna 'OriginalTweet' del dataset, tanto en train como test\n",
    "train_data_pandas['OriginalTweet'] = train_data_pandas['OriginalTweet'].apply(preprocess_text)\n",
    "test_data_pandas['OriginalTweet'] = test_data_pandas['OriginalTweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03ef59",
   "metadata": {},
   "source": [
    "Además vamos a eliminar caracteres especiales, como @ y # (nos quedaremos con otros, como \"!\", pues pueden ser importantes para el significado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f7dec991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar arrobas en direcciones de correo electrónico o menciones\n",
    "def preprocess_text2(text):\n",
    "    return re.sub(r'[@#]', '', text) #&\n",
    "\n",
    "# lo aplicamos\n",
    "train_data_pandas['OriginalTweet'] = train_data_pandas['OriginalTweet'].apply(preprocess_text2)\n",
    "test_data_pandas['OriginalTweet'] = test_data_pandas['OriginalTweet'].apply(preprocess_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d04cb",
   "metadata": {},
   "source": [
    "Veamos cómo quedaron los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "035411d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         menyrbie  phil_gahan  chrisitv http : //t.co/...\n",
       "1        advice talk neighbour family exchange phone nu...\n",
       "2        coronavirus australia : woolworth give elderly...\n",
       "3        food stock one empty ... please , n't panic , ...\n",
       "4        , ready go supermarket  covid19 outbreak . 'm ...\n",
       "                               ...                        \n",
       "41152    airline pilot offering stock supermarket shelf...\n",
       "41153    response complaint provided citing covid-19 re...\n",
       "41154    know itâs getting tough  kameronwilds rationi...\n",
       "41155    wrong smell hand sanitizer starting turn ?  co...\n",
       "41156     tartiicat well new/used rift going $ 700.00 a...\n",
       "Name: OriginalTweet, Length: 41157, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pandas['OriginalTweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29455bc6",
   "metadata": {},
   "source": [
    "#### Dividimos en train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3ad75c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "train_data, val_data = train_test_split(train_data_pandas, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825dfbcf",
   "metadata": {},
   "source": [
    "#### Vamos ahora a crear los datasets para trabajar con tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2756717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el DataFrame de pandas en un objeto tf.data.Dataset\n",
    "# armo según lo que me interesa\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data['OriginalTweet'].values, \n",
    "                                              train_data['Sentiment'].values))\n",
    "\n",
    "\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_data['OriginalTweet'].values, \n",
    "                                              val_data['Sentiment'].values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c3d0630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b'unemployment claim made online virginia week : monday : 426 tuesday : 2,150 number going get bigger . http : //t.co/fueg2rl2dl'\n",
      "label:  0\n"
     ]
    }
   ],
   "source": [
    "# veo un dato de train y uno de test\n",
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a6ae9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos para test\n",
    "\n",
    "# Cargar el DataFrame de pandas en un objeto tf.data.Dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data_pandas['OriginalTweet'].values, \n",
    "                                                   test_data_pandas['Sentiment'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f9381",
   "metadata": {},
   "source": [
    "Definimos el tamaño del buffer y del lote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "130c6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f050ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d07a18",
   "metadata": {},
   "source": [
    "Veamos algunos ejemplos y sus etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e3547c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'seem forgotten socially distance grocery store , let ( fake ) cough serve reminder .  backup  socialdistancing'\n",
      " b'result http : //t.co/m2i4rdpdg5 second survey consumer behavior . see difference 11 day make ... http : //t.co/qpdb6dxaty'\n",
      " b'time  coronavirus liquidating everything amazon low price . guitar hanger http : //t.co/78r5y5mhmb morale patch uniform http : //t.co/uu7bakobdu boonie hat http : //t.co/964xoyvjll pond net http : //t.co/3bilq0ssq6']\n",
      "\n",
      "labels: , [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print(f'texts:  {example.numpy()[:3]}\\n')\n",
    "  print(f'labels: , {label.numpy()[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40570385",
   "metadata": {},
   "source": [
    "Ahora crearemos y aplicaremos una capa llamada <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\" target='_blanck'>TextVectorization</a>, que quitará las puntuaciones, pasará todo a minúsculas y tokenizará:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ddb36375",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "max_length = 45 # max length our sequences will be (e.g. how many words from a Tweet does a model see?)\n",
    "\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)\n",
    "\n",
    "# Fit the text vectorizer instance to the training data using the adapt() method\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee364817",
   "metadata": {},
   "source": [
    "A continuación se muestran los primeros 20 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ada68128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'http', 'coronavirus', 'covid19', 'price', 'store',\n",
       "       'supermarket', 'food', 'grocery', 'people', 'amp', 'consumer',\n",
       "       '19', 'covid', 'shopping', 's', 'online', 'need', 'time'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d7f75",
   "metadata": {},
   "source": [
    "Ahora que el vocabulario está configurado, la capa puede codificar el texto en índices. Los tensores de índices son rellenados con 0s para que tengan el tamaño de la secuencia más larga en el lote.\n",
    "\n",
    "Veamos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c80a2ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1293, 3320, 3087,  375,    9,    6,  172,  835,  738, 1219, 1479,\n",
       "           1,   99,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 354,    2,    1,  673,  693,   12,  209,   64, 1416, 1276,   37,\n",
       "          54,    2,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  19,    3,    1,  210,  201,  134,    5,    1,    1,    2,    1,\n",
       "        9555, 5638, 5172,    2,    1,    1, 3978,    2,    1,    1, 3298,\n",
       "           2,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741a6cf",
   "metadata": {},
   "source": [
    "Vemos que rellena con 0s hasta tener siempre un largo de 45.\n",
    "\n",
    "Con esta configuración, el proceso no es completamente reversible (no hay un mapeo uno a uno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5d873bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  b'seem forgotten socially distance grocery store , let ( fake ) cough serve reminder .  backup  socialdistancing'\n",
      "Round-trip:  seem forgotten socially distance grocery store let fake cough serve reminder [UNK] socialdistancing                                \n",
      "\n",
      "Original:  b'result http : //t.co/m2i4rdpdg5 second survey consumer behavior . see difference 11 day make ... http : //t.co/qpdb6dxaty'\n",
      "Round-trip:  result http [UNK] second survey consumer behavior see difference 11 day make http [UNK]                               \n",
      "\n",
      "Original:  b'time  coronavirus liquidating everything amazon low price . guitar hanger http : //t.co/78r5y5mhmb morale patch uniform http : //t.co/uu7bakobdu boonie hat http : //t.co/964xoyvjll pond net http : //t.co/3bilq0ssq6'\n",
      "Round-trip:  time coronavirus [UNK] everything amazon low price [UNK] [UNK] http [UNK] morale patch uniform http [UNK] [UNK] hat http [UNK] [UNK] net http [UNK]                     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", example[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af48ea7",
   "metadata": {},
   "source": [
    "Puede observarse que hay muchos tokens desconocidos ([UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbdb90",
   "metadata": {},
   "source": [
    "Finalmente, aplicaremos una capa de embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a9a0f1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x7f0abc6a1630>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, # set the input shape; size of our vocabulary\n",
    "                                 output_dim=128, # set the size of the embedding vector\n",
    "                                 embeddings_initializer=\"uniform\", # default, initialize embedding vectors randomly\n",
    "                                 input_length=max_length # how long is each input\n",
    "                             )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf581ab",
   "metadata": {},
   "source": [
    "<a name=\"5\"> </a>\n",
    "## MODELOS\n",
    "\n",
    "Probaremos distinos modelos.\n",
    "\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c1eecc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para guardar los resultados y comparar después\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "41080333",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE=(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0fc1b",
   "metadata": {},
   "source": [
    "<a name=\"5.1\"> </a>\n",
    "### Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a1bcc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model_1(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape, dtype=tf.string) # inputs are 1-dimensional strings\n",
    "    x = encoder(inputs) # turn the input text into numbers \n",
    "    x = embedding(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = build_model_1(INPUT_SHAPE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "74dadf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 45)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 45, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "acacf449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "515/515 [==============================] - 11s 21ms/step - loss: 0.2310 - accuracy: 0.9184 - precision: 0.9207 - recall: 0.9515 - val_loss: 0.3726 - val_accuracy: 0.8540 - val_precision: 0.8579 - val_recall: 0.9199\n",
      "Epoch 2/5\n",
      "515/515 [==============================] - 10s 20ms/step - loss: 0.2149 - accuracy: 0.9232 - precision: 0.9252 - recall: 0.9544 - val_loss: 0.3801 - val_accuracy: 0.8540 - val_precision: 0.8713 - val_recall: 0.9006\n",
      "Epoch 3/5\n",
      "515/515 [==============================] - 10s 20ms/step - loss: 0.2014 - accuracy: 0.9293 - precision: 0.9323 - recall: 0.9564 - val_loss: 0.3911 - val_accuracy: 0.8517 - val_precision: 0.8690 - val_recall: 0.8994\n",
      "Epoch 4/5\n",
      "515/515 [==============================] - 11s 21ms/step - loss: 0.1905 - accuracy: 0.9336 - precision: 0.9366 - recall: 0.9587 - val_loss: 0.4021 - val_accuracy: 0.8513 - val_precision: 0.8718 - val_recall: 0.8948\n",
      "Epoch 5/5\n",
      "515/515 [==============================] - 10s 20ms/step - loss: 0.1816 - accuracy: 0.9373 - precision: 0.9409 - recall: 0.9601 - val_loss: 0.4182 - val_accuracy: 0.8517 - val_precision: 0.8625 - val_recall: 0.9087\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy', 'Precision','Recall'])\n",
    "\n",
    "# Fit the model\n",
    "history_1 = model_1.fit(train_dataset,\n",
    "                        #train_labels, \n",
    "                        epochs=5,\n",
    "                        validation_data=validation_dataset)\n",
    "                        #callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                        #                                       experiment_name=\"model_1_dense\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06393ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.8294 - precision: 0.8257 - recall: 0.8882\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score1 = model_1.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d592ce56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La precisión fue 83.0%, la precisión del 83.0% y el recall de 89.0%'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"La precisión fue {round(score1[1],2)*100}%, la precisión del {round(score1[2],2)*100}% y el recall de {round(score1[3],2)*100}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27c720",
   "metadata": {},
   "source": [
    "Guardamos los resultados para poder comparar después\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f2baf67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Model 1',\n",
       "  'accuracy': 0.829383909702301,\n",
       "  'precision': 0.8256762623786926,\n",
       "  'recall': 0.8882216811180115,\n",
       "  'f1-score': 0.85580773419097}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_1_results = {\n",
    "    'name': 'Model 1',\n",
    "    'accuracy':score1[1],\n",
    "    'precision':score1[2],\n",
    "    'recall':score1[3],\n",
    "    'f1-score': (2*(score1[2]*score1[3]))/(score1[2]+score1[3])\n",
    "}\n",
    "\n",
    "\n",
    "results.append(model_1_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232260e",
   "metadata": {},
   "source": [
    "#### Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bcfc3a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step\n",
      "pred: [[0.9958777]]\n",
      "Predicción: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "En la inferencia, la entrada debe pasar previamente por el pre-procesamiento (en este ejemplo\n",
    "por preprocess_text y pre-process_text2); no hace falta que pase por las caspas de text vectorization\n",
    "y embedding pues son parte del modelo\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Texto de entrada para hacer una predicción\n",
    "input_text = \"this is very, very positive\"\n",
    "\n",
    "\n",
    "# pre-procesamiento\n",
    "input_text = preprocess_text(input_text)\n",
    "input_text = preprocess_text2(input_text)\n",
    "\n",
    "\n",
    "# predicción\n",
    "pred = model_1.predict(np.array([input_text]))\n",
    "\n",
    "print(f\"pred: {pred}\")\n",
    "\n",
    "\n",
    "# Convertir la salida a una predicción binaria (0 o 1)\n",
    "binary_prediction = 1 if pred[0, 0] > 0.5 else 0\n",
    "\n",
    "# Imprimir la predicción\n",
    "print(\"Predicción:\", binary_prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c541a",
   "metadata": {},
   "source": [
    "<a name=\"5.2\"> </a>\n",
    "### Modelo 2 : LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5253b3",
   "metadata": {},
   "source": [
    "Arquitectura típica de una RNN:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/arq.png\" width=80%>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2cc37e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After embedding: (None, 45, 128)\n",
      "After LSTM cell: (None, 64)\n"
     ]
    }
   ],
   "source": [
    "def build_model_2(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape, dtype=\"string\")\n",
    "    x = encoder(inputs) # text vectorizer\n",
    "    x = embedding(x)\n",
    "    print(f\"After embedding: {x.shape}\")\n",
    "    # x = layers.LSTM(64, activation=\"tanh\", return_sequences=True)(x) # use return_sequences=True if you want to stack recurrent layers \n",
    "    # print(f\"After LSTM cell with return_sequences=True: {x.shape}\")\n",
    "    x = layers.LSTM(64, activation=\"tanh\")(x)\n",
    "    print(f\"After LSTM cell: {x.shape}\")\n",
    "    x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer to have on top of LSTM layer\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n",
    "    return model\n",
    "    \n",
    "\n",
    "model_2 = build_model_2(INPUT_SHAPE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d4ba0a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 45)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 45, 128)           1280000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,333,633\n",
      "Trainable params: 1,333,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2607dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "515/515 [==============================] - 10s 19ms/step - loss: 0.1462 - accuracy: 0.9526 - precision: 0.9561 - recall: 0.9686 - val_loss: 0.5118 - val_accuracy: 0.8434 - val_precision: 0.8614 - val_recall: 0.8946\n",
      "Epoch 2/5\n",
      "515/515 [==============================] - 10s 19ms/step - loss: 0.1421 - accuracy: 0.9538 - precision: 0.9576 - recall: 0.9691 - val_loss: 0.5276 - val_accuracy: 0.8393 - val_precision: 0.8674 - val_recall: 0.8783\n",
      "Epoch 3/5\n",
      "515/515 [==============================] - 10s 19ms/step - loss: 0.1383 - accuracy: 0.9547 - precision: 0.9584 - recall: 0.9696 - val_loss: 0.5439 - val_accuracy: 0.8384 - val_precision: 0.8737 - val_recall: 0.8683\n",
      "Epoch 4/5\n",
      "515/515 [==============================] - 11s 21ms/step - loss: 0.1355 - accuracy: 0.9574 - precision: 0.9613 - recall: 0.9709 - val_loss: 0.5687 - val_accuracy: 0.8411 - val_precision: 0.8499 - val_recall: 0.9072\n",
      "Epoch 5/5\n",
      "515/515 [==============================] - 11s 22ms/step - loss: 0.1319 - accuracy: 0.9588 - precision: 0.9624 - recall: 0.9722 - val_loss: 0.5769 - val_accuracy: 0.8370 - val_precision: 0.8675 - val_recall: 0.8739\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy', 'Precision','Recall'])\n",
    "\n",
    "# Fit the model\n",
    "history_2 = model_1.fit(train_dataset,\n",
    "                        epochs=5,\n",
    "                        validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "79bdbe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6931 - accuracy: 0.5700 - precision: 0.5701 - recall: 0.9995\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score2 = model_2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d0b91c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_results = {\n",
    "    'name': 'Model 2',\n",
    "    'accuracy':score2[1],\n",
    "    'precision':score2[2],\n",
    "    'recall':score2[3],\n",
    "    'f1-score': (2*(score2[2]*score2[3]))/(score2[2]+score2[3])\n",
    "}\n",
    "\n",
    "\n",
    "results.append(model_2_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3230a",
   "metadata": {},
   "source": [
    "<a name=\"5.3\"> </a>\n",
    "### Modelo 3 : GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4e9fec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_3(input_shape):\n",
    "    # Build an RNN using the GRU cell\n",
    "    inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = encoder(inputs)\n",
    "    x = embedding(x)\n",
    "    # x = layers.GRU(64, activation=\"tanh\", return_sequences=True)(x) # return_sequences=True is required for stacking recurrent cells\n",
    "    # print(x.shape)\n",
    "    x = layers.GRU(64, activation=\"tanh\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model= tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model_3 = build_model_3(INPUT_SHAPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1d269a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "515/515 [==============================] - 12s 23ms/step - loss: 0.1293 - accuracy: 0.9588 - precision: 0.9628 - recall: 0.9717 - val_loss: 0.5959 - val_accuracy: 0.8367 - val_precision: 0.8596 - val_recall: 0.8845\n",
      "Epoch 2/5\n",
      "515/515 [==============================] - 11s 22ms/step - loss: 0.1263 - accuracy: 0.9595 - precision: 0.9633 - recall: 0.9722 - val_loss: 0.6126 - val_accuracy: 0.8366 - val_precision: 0.8674 - val_recall: 0.8733\n",
      "Epoch 3/5\n",
      "515/515 [==============================] - 12s 22ms/step - loss: 0.1247 - accuracy: 0.9602 - precision: 0.9641 - recall: 0.9726 - val_loss: 0.6298 - val_accuracy: 0.8344 - val_precision: 0.8675 - val_recall: 0.8691\n",
      "Epoch 4/5\n",
      "515/515 [==============================] - 11s 22ms/step - loss: 0.1220 - accuracy: 0.9622 - precision: 0.9654 - recall: 0.9745 - val_loss: 0.6508 - val_accuracy: 0.8314 - val_precision: 0.8659 - val_recall: 0.8656\n",
      "Epoch 5/5\n",
      "515/515 [==============================] - 11s 21ms/step - loss: 0.1199 - accuracy: 0.9623 - precision: 0.9663 - recall: 0.9737 - val_loss: 0.6688 - val_accuracy: 0.8309 - val_precision: 0.8677 - val_recall: 0.8623\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy', 'Precision','Recall'])\n",
    "\n",
    "# Fit the model\n",
    "history_3 = model_1.fit(train_dataset,\n",
    "                        epochs=5,\n",
    "                        validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "013865ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 8ms/step - loss: 0.6917 - accuracy: 0.5700 - precision: 0.5700 - recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score3 = model_3.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5c9ae9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_results = {\n",
    "    'name': 'Model 3',\n",
    "    'accuracy':score3[1],\n",
    "    'precision':score3[2],\n",
    "    'recall':score3[3],\n",
    "    'f1-score': (2*(score3[2]*score3[3]))/(score3[2]+score3[3])\n",
    "}\n",
    "\n",
    "\n",
    "results.append(model_3_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbf2c5",
   "metadata": {},
   "source": [
    "<a name=\"5.4\"> </a>\n",
    "### Modelo 4 : Bidirectional RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f56ab",
   "metadata": {},
   "source": [
    "<img src=\"images/model4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "850e46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_4(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape, dtype=\"string\")\n",
    "    x = encoder(inputs)\n",
    "    x = embedding(x)\n",
    "    # x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # return_sequences=True required for stacking RNN layers\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"model_4_bidirectional\")\n",
    "    return model\n",
    "\n",
    "model_4 = build_model_4(INPUT_SHAPE)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3f6c3673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "515/515 [==============================] - 10s 19ms/step - loss: 0.1094 - accuracy: 0.9659 - precision: 0.9696 - recall: 0.9761 - val_loss: 0.7796 - val_accuracy: 0.8275 - val_precision: 0.8624 - val_recall: 0.8631\n",
      "Epoch 2/5\n",
      "515/515 [==============================] - 10s 20ms/step - loss: 0.1078 - accuracy: 0.9673 - precision: 0.9700 - recall: 0.9779 - val_loss: 0.7979 - val_accuracy: 0.8274 - val_precision: 0.8588 - val_recall: 0.8679\n",
      "Epoch 3/5\n",
      "515/515 [==============================] - 12s 23ms/step - loss: 0.1058 - accuracy: 0.9670 - precision: 0.9699 - recall: 0.9775 - val_loss: 0.8220 - val_accuracy: 0.8284 - val_precision: 0.8491 - val_recall: 0.8838\n",
      "Epoch 4/5\n",
      "515/515 [==============================] - 10s 20ms/step - loss: 0.1053 - accuracy: 0.9674 - precision: 0.9699 - recall: 0.9781 - val_loss: 0.8350 - val_accuracy: 0.8245 - val_precision: 0.8600 - val_recall: 0.8605\n",
      "Epoch 5/5\n",
      "515/515 [==============================] - 10s 20ms/step - loss: 0.1034 - accuracy: 0.9691 - precision: 0.9728 - recall: 0.9779 - val_loss: 0.8533 - val_accuracy: 0.8256 - val_precision: 0.8547 - val_recall: 0.8702\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_4.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy', 'Precision','Recall'])\n",
    "\n",
    "# Fit the model\n",
    "history_4 = model_1.fit(train_dataset,\n",
    "                        epochs=5,\n",
    "                        validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "db277251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 2s 12ms/step - loss: 0.6965 - accuracy: 0.3481 - precision: 0.3348 - recall: 0.1455\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score4 = model_4.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a25af72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_results = {\n",
    "    'name': 'Model 4',\n",
    "    'accuracy':score4[1],\n",
    "    'precision':score4[2],\n",
    "    'recall':score4[3],\n",
    "    'f1-score': (2*(score3[2]*score3[3]))/(score3[2]+score3[3])\n",
    "}\n",
    "\n",
    "\n",
    "results.append(model_4_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82409cee",
   "metadata": {},
   "source": [
    "<a name=\"5.5\"> </a>\n",
    "### Modelo 5 : Stacking layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19c816",
   "metadata": {},
   "source": [
    "<img src='images/model5.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "72143fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text vect. en la foto de arriba y la dense\n",
    "\n",
    "def build_model_5(input_shape,name):\n",
    "    inputs = layers.Input(shape=input_shape, dtype='string')\n",
    "    x = encoder(inputs)\n",
    "    x = embedding(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(32))(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    # dropout?\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=name)\n",
    "    return model\n",
    "    \n",
    "\n",
    "model_5 = build_model_5(INPUT_SHAPE, 'model_5')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "601a2472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "515/515 [==============================] - 12s 22ms/step - loss: 0.1021 - accuracy: 0.9692 - precision: 0.9719 - recall: 0.9790 - val_loss: 0.8725 - val_accuracy: 0.8250 - val_precision: 0.8536 - val_recall: 0.8706\n",
      "Epoch 2/5\n",
      "515/515 [==============================] - 10s 20ms/step - loss: 0.1004 - accuracy: 0.9696 - precision: 0.9728 - recall: 0.9788 - val_loss: 0.8957 - val_accuracy: 0.8235 - val_precision: 0.8511 - val_recall: 0.8714\n",
      "Epoch 3/5\n",
      "515/515 [==============================] - 10s 19ms/step - loss: 0.1001 - accuracy: 0.9694 - precision: 0.9723 - recall: 0.9791 - val_loss: 0.9130 - val_accuracy: 0.8234 - val_precision: 0.8538 - val_recall: 0.8673\n",
      "Epoch 4/5\n",
      "515/515 [==============================] - 10s 18ms/step - loss: 0.0979 - accuracy: 0.9704 - precision: 0.9739 - recall: 0.9790 - val_loss: 0.9318 - val_accuracy: 0.8216 - val_precision: 0.8595 - val_recall: 0.8557\n",
      "Epoch 5/5\n",
      "515/515 [==============================] - 10s 19ms/step - loss: 0.0976 - accuracy: 0.9700 - precision: 0.9731 - recall: 0.9791 - val_loss: 0.9511 - val_accuracy: 0.8224 - val_precision: 0.8540 - val_recall: 0.8652\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_5.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy', 'Precision','Recall'])\n",
    "\n",
    "# Fit the model\n",
    "history_5 = model_1.fit(train_dataset,\n",
    "                        epochs=5,\n",
    "                        validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "70886d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 4s 17ms/step - loss: 0.6933 - accuracy: 0.4916 - precision: 0.5717 - recall: 0.4309\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "score5 = model_5.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2f190203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_results = {\n",
    "    'name': 'Model 5',\n",
    "    'accuracy':score5[1],\n",
    "    'precision':score5[2],\n",
    "    'recall':score5[3],\n",
    "    'f1-score': (2*(score5[2]*score5[3]))/(score5[2]+score5[3])\n",
    "}\n",
    "\n",
    "\n",
    "results.append(model_5_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335bb64",
   "metadata": {},
   "source": [
    "<a name=\"5.6\"> </a>\n",
    "### Modelo 6 : Conv1D\n",
    "\n",
    "We've seen before how convolutional neural networks can be used for images but they can also be used for text.\n",
    "\n",
    "Previously we've used the layer Conv2D (which is great for images with (height, width)).\n",
    "\n",
    "But if we want to use convolutional layers for sequences (e.g. text) we need to use Conv1D: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D\n",
    "\n",
    "For more of a deep dive into what goes on behind the scenes in a CNN for text (or sequences) see the paper: https://arxiv.org/abs/1809.08037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
